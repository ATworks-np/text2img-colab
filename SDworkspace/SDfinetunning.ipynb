{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"_knQmT61uz3x"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","%cd '/content/drive/My Drive/SDworkspace'"]},{"cell_type":"code","source":["from PIL import Image\n","import os\n","\n","def image_grid(imgs, rows, cols):\n","    assert len(imgs) == rows*cols\n","\n","    w, h = imgs[0].size\n","    grid = Image.new('RGB', size=(cols*w, rows*h))\n","    grid_w, grid_h = grid.size\n","    \n","    for i, img in enumerate(imgs):\n","        grid.paste(img, box=(i%cols*w, i//cols*h))\n","    return grid\n","\n","dir = './images'\n","save_path = \"./my_concept\"\n","filenames = os.listdir(dir)\n","images = []\n","for filename in filenames:\n","  img = Image.open(f\"{dir}/{filename}\")\n","  img= img.resize((512, 512))\n","  img.save(f\"{save_path}/{filename}\")\n","  images.append(img)\n","\n","image_grid(images, 1, len(images))"],"metadata":{"id":"DP-O2m9dNOKI"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cEJP0OOSu94n"},"outputs":[],"source":["!pip install -qq diffusers[\"training\"]==0.3.0 transformers ftfy\n","!pip install -qq \"ipywidgets>=7,<8\"\n","\n","from huggingface_hub import notebook_login\n","notebook_login()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SrW0DJr3viBv"},"outputs":[],"source":["import argparse\n","import itertools\n","import math\n","import os\n","import random\n","\n","import numpy as np\n","import torch\n","import torch.nn.functional as F\n","import torch.utils.checkpoint\n","from torch.utils.data import Dataset\n","\n","import PIL\n","from accelerate import Accelerator\n","from accelerate.logging import get_logger\n","from accelerate.utils import set_seed\n","from diffusers import AutoencoderKL, DDPMScheduler, PNDMScheduler, StableDiffusionPipeline, UNet2DConditionModel\n","from diffusers.hub_utils import init_git_repo, push_to_hub\n","from diffusers.optimization import get_scheduler\n","from diffusers.pipelines.stable_diffusion import StableDiffusionSafetyChecker\n","from PIL import Image\n","from torchvision import transforms\n","from tqdm.auto import tqdm\n","from transformers import CLIPFeatureExtractor, CLIPTextModel, CLIPTokenizer"]},{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1663514737214,"user":{"displayName":"ぬぽまる","userId":"10678939553048559984"},"user_tz":-540},"id":"zfLMlEU4vwDj"},"outputs":[],"source":["# 事前学習モデルのパス\n","pretrained_model_name_or_path = \"CompVis/stable-diffusion-v1-4\"\n","\n","# 学習内容 (object or style)\n","what_to_teach = \"style\"\n","\n","# 新しい概念を表す単語\n","placeholder_token = \"<kutani-ware>\"\n","\n","# 新しい概念が何であるかを要約する単語\n","initializer_token = \"art\"\n","\n","# 学習用のプロンプトテンプレート\n","imagenet_templates_small = [\n","    \"a photo of a {}\",\n","    \"a rendering of a {}\",\n","    \"a cropped photo of the {}\",\n","    \"the photo of a {}\",\n","    \"a photo of a clean {}\",\n","    \"a photo of a dirty {}\",\n","    \"a dark photo of the {}\",\n","    \"a photo of my {}\",\n","    \"a photo of the cool {}\",\n","    \"a close-up photo of a {}\",\n","    \"a bright photo of the {}\",\n","    \"a cropped photo of a {}\",\n","    \"a photo of the {}\",\n","    \"a good photo of the {}\",\n","    \"a photo of one {}\",\n","    \"a close-up photo of the {}\",\n","    \"a rendition of the {}\",\n","    \"a photo of the clean {}\",\n","    \"a rendition of a {}\",\n","    \"a photo of a nice {}\",\n","    \"a good photo of a {}\",\n","    \"a photo of the nice {}\",\n","    \"a photo of the small {}\",\n","    \"a photo of the weird {}\",\n","    \"a photo of the large {}\",\n","    \"a photo of a cool {}\",\n","    \"a photo of a small {}\",\n","]\n","\n","imagenet_style_templates_small = [\n","    \"a painting in the style of {}\",\n","    \"a rendering in the style of {}\",\n","    \"a cropped painting in the style of {}\",\n","    \"the painting in the style of {}\",\n","    \"a clean painting in the style of {}\",\n","    \"a dirty painting in the style of {}\",\n","    \"a dark painting in the style of {}\",\n","    \"a picture in the style of {}\",\n","    \"a cool painting in the style of {}\",\n","    \"a close-up painting in the style of {}\",\n","    \"a bright painting in the style of {}\",\n","    \"a cropped painting in the style of {}\",\n","    \"a good painting in the style of {}\",\n","    \"a close-up painting in the style of {}\",\n","    \"a rendition in the style of {}\",\n","    \"a nice painting in the style of {}\",\n","    \"a small painting in the style of {}\",\n","    \"a weird painting in the style of {}\",\n","    \"a large painting in the style of {}\",\n","]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"u-ash-jvwRvd"},"outputs":[],"source":["# データセットの準備\n","class TextualInversionDataset(Dataset):\n","    def __init__(\n","        self,\n","        data_root,\n","        tokenizer,\n","        learnable_property=\"style\",  # [object, style]\n","        size=512,\n","        repeats=100,\n","        interpolation=\"bicubic\",\n","        flip_p=0.5,\n","        set=\"train\",\n","        placeholder_token=\"*\",\n","        center_crop=False,\n","    ):\n","\n","        self.data_root = data_root\n","        self.tokenizer = tokenizer\n","        self.learnable_property = learnable_property\n","        self.size = size\n","        self.placeholder_token = placeholder_token\n","        self.center_crop = center_crop\n","        self.flip_p = flip_p\n","\n","        self.image_paths = [os.path.join(self.data_root, file_path) for file_path in os.listdir(self.data_root)]\n","\n","        self.num_images = len(self.image_paths)\n","        self._length = self.num_images\n","\n","        if set == \"train\":\n","            self._length = self.num_images * repeats\n","\n","        self.interpolation = {\n","            \"linear\": PIL.Image.LINEAR,\n","            \"bilinear\": PIL.Image.BILINEAR,\n","            \"bicubic\": PIL.Image.BICUBIC,\n","            \"lanczos\": PIL.Image.LANCZOS,\n","        }[interpolation]\n","\n","        self.templates = imagenet_style_templates_small if learnable_property == \"style\" else imagenet_templates_small\n","        self.flip_transform = transforms.RandomHorizontalFlip(p=self.flip_p)\n","\n","    def __len__(self):\n","        return self._length\n","\n","    def __getitem__(self, i):\n","        example = {}\n","        image = Image.open(self.image_paths[i % self.num_images])\n","\n","        if not image.mode == \"RGB\":\n","            image = image.convert(\"RGB\")\n","\n","        placeholder_string = self.placeholder_token\n","        text = random.choice(self.templates).format(placeholder_string)\n","\n","        example[\"input_ids\"] = self.tokenizer(\n","            text,\n","            padding=\"max_length\",\n","            truncation=True,\n","            max_length=self.tokenizer.model_max_length,\n","            return_tensors=\"pt\",\n","        ).input_ids[0]\n","\n","        # default to score-sde preprocessing\n","        img = np.array(image).astype(np.uint8)\n","\n","        if self.center_crop:\n","            crop = min(img.shape[0], img.shape[1])\n","            h, w, = (\n","                img.shape[0],\n","                img.shape[1],\n","            )\n","            img = img[(h - crop) // 2 : (h + crop) // 2, (w - crop) // 2 : (w + crop) // 2]\n","\n","        image = Image.fromarray(img)\n","        image = image.resize((self.size, self.size), resample=self.interpolation)\n","\n","        image = self.flip_transform(image)\n","        image = np.array(image).astype(np.uint8)\n","        image = (image / 127.5 - 1.0).astype(np.float32)\n","\n","        example[\"pixel_values\"] = torch.from_numpy(image).permute(2, 0, 1)\n","        return example"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gCt0zR-XwWoE"},"outputs":[],"source":["# トークナイザーの準備\n","tokenizer = CLIPTokenizer.from_pretrained(\n","    pretrained_model_name_or_path,\n","    subfolder=\"tokenizer\",\n","    use_auth_token=True,\n",")\n","\n","# トークナイザーにプレースホルダートークンを追加\n","num_added_tokens = tokenizer.add_tokens(placeholder_token)\n","if num_added_tokens == 0:\n","    raise ValueError(\n","        f\"The tokenizer already contains the token {placeholder_token}. Please pass a different\"\n","        \" `placeholder_token` that is not already in the tokenizer.\"\n","    )\n","\n","# トークンIDの準備\n","token_ids = tokenizer.encode(initializer_token, add_special_tokens=False)\n","\n","# 単一トークンと複数トークンのどちらであるか確認\n","if len(token_ids) > 1:\n","    raise ValueError(\"The initializer token must be a single token.\")\n","\n","initializer_token_id = token_ids[0]\n","placeholder_token_id = tokenizer.convert_tokens_to_ids(placeholder_token)\n","\n","# Stable Diffusionモデルの準備\n","text_encoder = CLIPTextModel.from_pretrained(\n","    pretrained_model_name_or_path, subfolder=\"text_encoder\", use_auth_token=True\n",")\n","vae = AutoencoderKL.from_pretrained(\n","    pretrained_model_name_or_path, subfolder=\"vae\", use_auth_token=True\n",")\n","unet = UNet2DConditionModel.from_pretrained(\n","    pretrained_model_name_or_path, subfolder=\"unet\", use_auth_token=True\n",")\n","# トークンの特徴ベクトルのサイズ変更\n","text_encoder.resize_token_embeddings(len(tokenizer))\n","# プレースホルダートークンの初期化\n","token_embeds = text_encoder.get_input_embeddings().weight.data\n","token_embeds[placeholder_token_id] = token_embeds[initializer_token_id]\n","\n","def freeze_params(params):\n","    for param in params:\n","        param.requires_grad = False\n","\n","# vaeとunetをフリーズ\n","freeze_params(vae.parameters())\n","freeze_params(unet.parameters())\n","\n","# テキストエンコーダのトークン特徴ベクトルを除くすべてのパラメータをフリーズ\n","params_to_freeze = itertools.chain(\n","    text_encoder.text_model.encoder.parameters(),\n","    text_encoder.text_model.final_layer_norm.parameters(),\n","    text_encoder.text_model.embeddings.position_embedding.parameters(),\n",")\n","freeze_params(params_to_freeze)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s3d7_o97wphw"},"outputs":[],"source":["# データセットの準備\n","train_dataset = TextualInversionDataset(\n","      data_root=save_path,\n","      tokenizer=tokenizer,\n","      size=512,\n","      placeholder_token=placeholder_token,\n","      repeats=100,\n","      learnable_property=what_to_teach, # object or style\n","      center_crop=False,\n","      set=\"train\",\n",")\n","\n","# データローダーの準備\n","def create_dataloader(train_batch_size=1):\n","    return torch.utils.data.DataLoader(train_dataset, batch_size=train_batch_size, shuffle=True)\n","\n","# noise_schedulerの準備\n","noise_scheduler = DDPMScheduler(\n","    beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", num_train_timesteps=1000, tensor_format=\"pt\"\n",")\n","# ハイパーパラメータの準備\n","hyperparameters = {\n","    \"learning_rate\": 5e-04,\n","    \"scale_lr\": True,\n","    \"max_train_steps\": 3000,\n","    \"train_batch_size\": 1,\n","    \"gradient_accumulation_steps\": 4,\n","    \"seed\": 42,\n","    \"output_dir\": \"sd-concept-output\"\n","}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0v9WdznFw5aV"},"outputs":[],"source":["# 学習の実行\n","def training_function(text_encoder, vae, unet):\n","    logger = get_logger(__name__)\n","\n","    train_batch_size = hyperparameters[\"train_batch_size\"]\n","    gradient_accumulation_steps = hyperparameters[\"gradient_accumulation_steps\"]\n","    learning_rate = hyperparameters[\"learning_rate\"]\n","    max_train_steps = hyperparameters[\"max_train_steps\"]\n","    output_dir = hyperparameters[\"output_dir\"]\n","\n","    accelerator = Accelerator(\n","        gradient_accumulation_steps=gradient_accumulation_steps,\n","    )\n","\n","    train_dataloader = create_dataloader(train_batch_size)\n","\n","    if hyperparameters[\"scale_lr\"]:\n","        learning_rate = (\n","            learning_rate * gradient_accumulation_steps * train_batch_size * accelerator.num_processes\n","        )\n","\n","    # optimizerの初期化\n","    optimizer = torch.optim.AdamW(\n","        text_encoder.get_input_embeddings().parameters(),  # only optimize the embeddings\n","        lr=learning_rate,\n","    )\n","\n","\n","    text_encoder, optimizer, train_dataloader = accelerator.prepare(\n","        text_encoder, optimizer, train_dataloader\n","    )\n","\n","    # vaeとunetをデバイスに移動\n","    vae.to(accelerator.device)\n","    unet.to(accelerator.device)\n","\n","    # 学習しないためvaeとunetをevalモデルに保持\n","    vae.eval()\n","    unet.eval()\n","\n","    # データローダーのサイズが変更された可能性があるため、学習ステップの合計を再計算する必要がある\n","    num_update_steps_per_epoch = math.ceil(len(train_dataloader) / gradient_accumulation_steps)\n","    num_train_epochs = math.ceil(max_train_steps / num_update_steps_per_epoch)\n","\n","    # 学習\n","    total_batch_size = train_batch_size * accelerator.num_processes * gradient_accumulation_steps\n","\n","    logger.info(\"***** Running training *****\")\n","    logger.info(f\"  Num examples = {len(train_dataset)}\")\n","    logger.info(f\"  Instantaneous batch size per device = {train_batch_size}\")\n","    logger.info(f\"  Total train batch size (w. parallel, distributed & accumulation) = {total_batch_size}\")\n","    logger.info(f\"  Gradient Accumulation steps = {gradient_accumulation_steps}\")\n","    logger.info(f\"  Total optimization steps = {max_train_steps}\")\n","    # 各マシンで1回だけプログレスバーを表示\n","    progress_bar = tqdm(range(max_train_steps), disable=not accelerator.is_local_main_process)\n","    progress_bar.set_description(\"Steps\")\n","    global_step = 0\n","\n","    for epoch in range(num_train_epochs):\n","        text_encoder.train()\n","        for step, batch in enumerate(train_dataloader):\n","            with accelerator.accumulate(text_encoder):\n","                # 画像を潜在空間に変換\n","                latents = vae.encode(batch[\"pixel_values\"]).latent_dist.sample().detach()\n","                latents = latents * 0.18215\n","\n","                # 潜在変数に追加するサンプルノイズ\n","                noise = torch.randn(latents.shape).to(latents.device)\n","                bsz = latents.shape[0]\n","                # Sample a random timestep for each image\n","                timesteps = torch.randint(0, noise_scheduler.num_train_timesteps, (bsz,), device=latents.device).long()\n","\n","                # 各タイムステップでのノイズの大きさに応じて潜在変数にノイズを追加\n","                # (これは前方拡散プロセス)\n","                noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)\n","\n","                # 条件付け用のテキスト埋め込みを取得\n","                encoder_hidden_states = text_encoder(batch[\"input_ids\"])[0]\n","\n","                # ノイズの残差を予測\n","                noise_pred = unet(noisy_latents, timesteps, encoder_hidden_states).sample\n","\n","                loss = F.mse_loss(noise_pred, noise, reduction=\"none\").mean([1, 2, 3]).mean()\n","                accelerator.backward(loss)\n","\n","                # コンセプトの埋め込みのみを最適化したいので、\n","                # 追加された埋め込みを除くすべてのトークン埋め込みの勾配をゼロにする\n","                if accelerator.num_processes > 1:\n","                    grads = text_encoder.module.get_input_embeddings().weight.grad\n","                else:\n","                    grads = text_encoder.get_input_embeddings().weight.grad\n","                # gradsをゼロにしたいトークンのインデックスを取得\n","                index_grads_to_zero = torch.arange(len(tokenizer)) != placeholder_token_id\n","                grads.data[index_grads_to_zero, :] = grads.data[index_grads_to_zero, :].fill_(0)\n","\n","                optimizer.step()\n","                optimizer.zero_grad()\n","\n","            # アクセラレータがバックグラウンドで最適化ステップを実行したかどうかを確認\n","            if accelerator.sync_gradients:\n","                progress_bar.update(1)\n","                global_step += 1\n","\n","            logs = {\"loss\": loss.detach().item()}\n","            progress_bar.set_postfix(**logs)\n","\n","            if global_step >= max_train_steps:\n","                break\n","\n","        accelerator.wait_for_everyone()\n","\n","\n","    # 学習済みモジュールを使用してパイプラインを作成し保存\n","    if accelerator.is_main_process:\n","        pipeline = StableDiffusionPipeline(\n","            text_encoder=accelerator.unwrap_model(text_encoder),\n","            vae=vae,\n","            unet=unet,\n","            tokenizer=tokenizer,\n","            scheduler=PNDMScheduler(\n","                beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", skip_prk_steps=True\n","            ),\n","            safety_checker=StableDiffusionSafetyChecker.from_pretrained(\"CompVis/stable-diffusion-safety-checker\"),\n","            feature_extractor=CLIPFeatureExtractor.from_pretrained(\"openai/clip-vit-base-patch32\"),\n","        )\n","        pipeline.save_pretrained(output_dir)\n","        # 新しく学習された埋め込みも保存\n","        learned_embeds = accelerator.unwrap_model(text_encoder).get_input_embeddings().weight[placeholder_token_id]\n","        learned_embeds_dict = {placeholder_token: learned_embeds.detach().cpu()}\n","        torch.save(learned_embeds_dict, os.path.join(output_dir, \"learned_embeds.bin\"))\n","import accelerate\n","accelerate.notebook_launcher(training_function, args=(text_encoder, vae, unet))"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMzcbqDURsDK/2Xc0SbH7kS"},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}